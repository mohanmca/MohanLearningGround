but I'm also going to talk about how we
design the algorithm which is perhaps
more
self and I should say this has work done
jointly with Jago Ungaro in fact this
formed the basis for his PhD
dissertation so a lot of what I'm
telling you about is actually work that
Diego has done so to start off when you
design a new algorithm because the
question is how are you going to
evaluate what you did in that algorithm
and there's some obvious answers to that
you know you're probably going to
evaluate your algorithm based on whether
it actually does what you want it to do
is it correct is it efficient maybe
you'll evaluate it based on how concise
and clean the specification is but I'm
going to argue today that you should
also evaluate algorithms based on how
understandable they are how simple and
easy it is for somebody to grasp the
algorithm and I have to say I don't
think this is commonly done in academia
today in fact probably the opposite that
we think the more confusing our
algorithm is the more clever we must be
in order to create this thing if we can
actually make it work it it's super
confusing we must be really really smart
but algorithms need to be understandable
because typically it's hard to get
benefit from an algorithm unless it's
actually implemented and the process of
implementing an algorithm always changes
it it always has to get extended or
adapted or modified in some way to fit
its environment and if the people that
are implementing the algorithm can't
understand it don't have good intuitions
about it they're not going to be able to
implement it properly and won't be able
to achieve the benefits of the algorithm
and this is particularly true in
distributed systems where the algorithms
are pretty darn complicated to begin
with so today I'm going to talk about
this issue in the context of consensus
algorithms a consensus algorithms I
think are probably the most fundamental
important algorithms in all of
distributed systems they allow a
collection of machines to somehow
operate in a reasonable lockstep where
they all a group actually operates as if
it's a single machine so that we can
take a collection of relatively
unreliable machines and make them behave
like one super reliable machine so it
can continue to offer service even as
some of the these unreliable components
fail and when it comes to consensus
there's been one algorithm has really
pretty much all implementations of
consensus since then have been based on
Paxos and people learn paxos in school
and so on the problem with paxos is it's
really really hard to understand a wrap
your mind around then furthermore the
basic version of pack says that
everybody starts with this really only
solves a tiny part of the problem you
really need to solve if you're going to
use consensus algorithms and so it's
proven really difficult to build systems
many people have tried and it's been
very very difficult so today I'm going
to talk about a new consensus algorithm
that Diego and I designed called raft
and it had the unusual design goal that
our number one goal was
understandability we wanted to make an
algorithm that was easier to understand
so you could develop intuitions about it
easy to explain and so on and then in
addition we also wanted something
complete enough that you could really
use it to build real systems it covered
enough of the problems that it's really
simplified building real systems and as
you're going to see this resulted in a
very different problem decomposition
from what Paxos has so today what I'm
going to do is talk about raft but I'm
also going to talk about how we designed
for understandability what does that
mean and how do you actually do that and
how did that influence the design of the
of the raft algorithm and I'll talk a
little bit at the end about a user study
we did that shows that we think that
shows pretty compelling evidence that in
fact raft is more understandable in
Paxos and I'll also talk a little bit
about the adoption of raft so but first
some background so consensus algorithms
are typically used in the context of
what's called replicated state machines
now before I talk about replicated state
machines let me step back even further
and talk about a state machine so when I
say state machine what I really mean is
something typically a program that
responds to some sort of external
stimuli such as commands coming from
clients and it manages some internal
state about that so this is a pretty
general model of computation that
includes a lot of the services that we
use in today's large-scale data center
applications so storage systems like
memcache D or the RAM cloud project I've
been working on these are examples of
state machines the name node for each
DFS most of the things we think of as
services today you can think of as a
statement
now the question is how do we build
really really reliable state machines
and the model that seems to be most
popular for this is a replicated state
machine the idea is you take the state
machine and you run the same state
machine on several different machines
and the goal is that these all kind of
run in lockstep which means that each of
the three of these state machines
execute exactly the same set of commands
that is it gets the same stimuli in
exactly the same order and if what
happens then they all ought to produce
exactly the same results state machines
I forgot to mention they need to be
deterministic so if you have that
they'll all produce the same result and
what we want to do is I make this so
reliable that it can survive the failure
of any of these machines so the way that
happens in practice is to use a
replicated log and all of the commands
for the state machine get added to this
replicated log before they are executed
by the state machines so as long as we
keep the logs identical those state
machines will execute the same commands
produce the same results so when a new
command is submitted to the system so
for example this command sets Z the to
the value variable x before that machine
can execute it it first has to replicate
it into all of the logs and so it passes
the command off to the other machines in
the cluster
everyone adds that command to the end of
their logs so the command has to appear
at exactly the same position in all of
the logs and then once the command has
been properly replicated in the logs
then it can get executed by the state
machines and the original state machine
returns the answer back to the client
and the idea behind this if we do this
right then
this cluster can continue functioning as
long as any majority of the machines are
up so with a two node cluster as long as
two out of three nodes are up it can
keep running and provide full service so
consensus the consensus module is the
thing that's responsible for replicating
these commands into the replicated log
so that we end up with the logs
identical across all of the machines and
that's what I'm going to talk about
today now is how do we actually
implement that replicated log and if we
do this right again it will work fine as
long as we have
the machines up and it will survive the
common failure models that we see in
systems today such as messages that are
loss of communication errors machine
crashes as long as the machines don't
turn evil the Machine simply stops and
crashes this will work fine and it can
handle machines that are down and by the
way I'm happy to take questions along
the way in the talk no need to wait till
the end if we start running late on time
I'll I'll cut questions off at the end
of feel free to jump in with questions
so before I go on any questions so far
about the basic thing that we're trying
to implement all right so how do we do
that well as I said the the way that's
been done historically is to use Paxos
for this and Paxos as this Paxos comes
in several different flavors but the the
most commonly studied flavor is called
single Decree Paxos and this is a
mechanism by which a collection of
machines can agree on a single value one
value over the entire life of the system
once and for all agree on one value so
there's a collection of one or more
machines that propose values and then we
have a collection of other machines that
will determine which value is accepted
and the idea is we need to accept
exactly one of the proposed values and
it works in a two-phase process seems
like anything involving a distributive
system always has two phases two
proposed phase an accept phase so the
proposers I'm not going to go over the
algorithm great detail because I would
take my whole hour but the basic ideas
proposes pick some proposal number
bigger than anything they've ever seen
before and then they send that out to
all of the acceptors in the proposed
phase and each acceptor checks to see if
if has it ever seen a proposal number
that's greater than that before and if
this is the biggest value it's ever seen
then it responds otherwise in Paxos the
acceptor assembly doesn't respond if the
proposer gets back responses from a
majority of the acceptors then it goes
the second phase where it sends out that
proposal number plus value to all of the
acceptors that ask them to formally
accept that value and the value that it
sends out is one of two things first
during the accept phase if any of those
acceptors had already accepted a value
previously it returns
that and the proposer will pick the
accepted value with the highest proposal
number which everyone's highest it has
to use that one in the second phase if
none of the acceptors had ever seen any
accepted of value before then the
proposer gets to use its own value it
sends that out to the acceptors again
they check the proposal number to see if
that's as big as anything they've ever
seen and if if this proposal number is
at least as large as anything else then
they accept and finally if the proposer
gets back a majority there then that
value has been chosen so that's the
basic Paxos algorithm it's actually
seems pretty simple it's certainly very
concise not a lot to it it's been proven
correct you know what could possibly be
wrong with this so what's wrong with
this is that this algorithm is really
hard to actually wrap your mind around
and understand you know why does this
work and for example if we go back up
what's to keep it looks like an acceptor
can accept one value and then later on
accept a different value how do we know
this ever converges and that we really
only reach consensus on a single value
and why is this a greater than or equal
to but that's a greater than if you
start trying to dig in and understand
this it gets pretty hard to actually
figure out what's going on and
furthermore that's only the beginnings
of a replicated log this only agrees on
one value for the entire life of the
universe so to make a replicated log you
somehow have to have a bunch of
agreements on different values and
stitch them all together into a log and
by the way that algorithm for the
priests page doesn't address aliveness
at any point along the way somebody can
just stop responding so it doesn't
actually guarantee that we converge on a
value it just make sure that we if we
converge there's only one value and by
the way how do we choose the proposal
numbers and what about membership
management and so if you think that the
basic paxos is hard it gets really
complicated by the time you add all the
other machinery to do a replicated log
so let me just try a quick survey here
how many of you have tried to understand
Paxos at some point in your professional
career okay and now those of you the
tribe if you found it difficult to
understand keep your hand up okay now
those of you that at the end
you got kung fu you figure it out you're
comfortable you feel like you could
explain it to somebody else now or
implement it relatively reliably if
that's you keep your hand up now I might
keep your hand up if that's you yeah
nobody's hands are up still it's really
hard so by the way there are solutions
to all these other problems and there's
also a problem with taxes that the base
form is more expensive than it needs to
be and there's a solution to that also
but there's no agreement on any of these
solutions
you know there's papers have been
written on Paxos made simple and tax
laws made complete and Paxos made
complicated taxes made practical there's
no agreement any of this stuff so if you
want to build a system with paxos the
ironic thing is you have to take the
simple algorithm and then add all this
additional stuff in which there's no
agreement and change the fundamental
architecture to make it more efficient
and at the same time you can't
understand how it works so what are the
odds anybody's actually going to be able
to do that successfully
and the answer is not very many people
have been able to do it successfully in
fact personally Paxos just drove me
crazy
so in RAM cloud at one point we decided
we wanted to use consensus to manage our
top-level configuration information for
a ram quad cluster then diego Ungaro
sort of looking into this and said what
we should use Paxos that's what
everybody is using it's got to be the
best thing so I said fine but you have
to explain to me how Paxos works because
I don't yet understand how Paxos works
so Diego came into my office and gave it
his best shot one day and explained it
to me and I couldn't understand I had a
whole bunch of questions he couldn't
answer my questions so I said okay let's
try again next week and so next week get
back we try it again still couldn't
understand it so then I had him write
the algorithm on my board and my
whiteboard every day I would come into
my office and I would look at that and
stare at that and see if I could make
any sense of this it's basically only
six or eight lines of code how can you
not understand that and I still couldn't
get it I read the papers
I read the proof of correctness of Paxos
in Paxos made simple and I could
understand the proof enough to convince
myself that the algorithm worked I mean
I believe that it worked and yet I
couldn't
at how I still the code still made no
sense to me in fact I didn't actually
really understand paxos fully until
after we developed raft was finally they
had to basically build a new consensus
algorithm to figure out how paxos worked
so I started thinking well I think I
must be of at least average intelligence
among the computer science community and
so that must mean there's got to be a
bunch of other people this half of the
other computer scientists in the world
are probably about as confused as I am
about taxes is there something we can do
to make this better is this really the
best we can do so finally I decided I'm
just going to try and write a new
consensus algorithm from scratch and
maybe that way I'll understand it maybe
you know maybe Paxos is inevitably the
only way to do consensus and I do this
I'm going to find the only way I can
make things work is eventually to
recreate paxos so I started working on
that and came up with some ideas for
what's what seem much simpler and I
showed it to the RAM cloud students
turned out what I'd done at the time was
actually hopelessly wrong was broken in
a whole bunch of different ways but we
had the delusion at least at first that
we could do something simpler in Paxos
and that got diego interested in the
project and so then we spent the next
six months really working together
designing a consensus algorithm and our
fundamental goal was we wanted to see
what's the simplest most understandable
algorithm we can possibly develop and
then we had to feel what does that mean
to be more understandable and how do we
judge whether one algorithm is more
understandable than another and then
what techniques would you use to do that
and this is all stuff we just kind of
tried different things along the way and
figure this out as we went but the main
criterion we use is when choosing
between our two algorithms we would try
explaining them and we'd pick the one
that was simplest to explain and we
tried if something was hard to explain
then we throw that one out throw it away
see if we could find some other
algorithm that was simpler to explain
and then over time we started realizing
there were certain techniques we were
using that tended to make things more
understandable whom the first one is so
this is the most important thing in all
of computer science problem
decomposition we try and subdivide the
problem into pieces that you can
understand relatively
don't interact too much with each other
and then the second one was minimizing
the state space you can think of that as
getting rid of as many if statements as
possible in the code one way to think
about it so for example try and develop
a single mechanism that will solve
multiple problems so you don't have
separate mechanisms to learn and
understand eliminating special cases
make everything fall out of the common
case make the common case just do the
right thing so you don't have any
special cases or an instrument system so
maximizing your consistency and
minimizing the non determinism in the
system you can't completely eliminate
them but you can't make it perfectly
coherent and completely eliminate non
determinism but get rid of those at all
the stages so what I'm going to do now
is I'm going to describe raft for you
then we try and interject at various
points where we make decisions based on
understandability and how that factored
into the decision process and I should
warn you I don't want to set your
expectations too high you know raft is
still not trivial it's not like we've
suddenly made consensus falling over
simple consensus is a hard problem but
we believe we've actually made something
considerably simpler than what was there
before so again it's not going to be
can't make it totally simple but I'm
hoping that by the end of this talk
you'll understand most of the basic
ideas about how to be a consensus and
and replicated logs so the first thing I
said the technique we use for making
things simple is decomposition so with
Raph we decompose the problem in a very
different way with Paxos they first did
this teeny tiny complete consensus
that's the single decree Paxos and then
try to accumulate a bunch of those into
a log in raft we took a different
approach a much more asymmetric approach
where we first pick a leader pick one of
those machines at any given time is in
complete utter dictatorial charge and
then if that leader crashes that we pick
a new one so first problem is leader
election and the second problem is how
does the leader operate during the
normal behavior of the system so it
receives entries from commands from
clients app ends them to its log and
then pushes those entries out to all the
other logs so I'll talk a little bit
about how that works and then the third
problem is safety which is
what happens when leaders crash because
they could be halfway through
replicating an entry and then they crash
and so some servers have it and that
entry and some servers don't have that
and so I'll show first one simple
addition we made that keeps the logs
consistent so they if they get out of
whack they will get back in whack very
quickly and then second a little
modification to the election process so
that we only elect certain machines as
servers you can only be server if you
have a reasonably up-to-date log so I'll
talk about each of these separately but
first I want to give you just a quick
demo to show you how the system works so
this is a visualization tool called raft
scope that Diego built it's available on
the internet by the way you can google
it or go to the raft homepage on github
and see this if it's great if you want
to play around with raft and see how it
works here we have a cluster with five
servers and the five blobs here and then
we have there are five logs here which
start off empty so the way raft works is
that the servers expect there to be a
leader and so when it first starts up
everybody is waiting around to hear
instructions from a leader there is no
leader but they all have timeouts and
then those are those little rings that
are shrinking around the outside and
eventually one of the servers will
timeout and then it tries to become
leader and so will send out requests to
all of the other servers in the cluster
asking them if it can become leader and
if it gets back a response to those in
fact we actually had two servers that
timed out about almost the same time one
of them won the election and became
leader and now that's that's server five
and every once in a while it sends out
heartbeat messages to everybody else so
they know there's a leader there and you
can see when they get a heartbeat
message they reset all of their little
timeout rings again so now this leader
is in control of the cluster now suppose
a client comes in and issues a command
center that I'll pop up a little window
and send a request so if this server
five has received a request and it's
added that to its log and the dotted
line means around this means this entry
has not been replicated safely enough
for us to actually execute it yet hasn't
been replicated if I resume the cluster
this server will send out messages to
replicate that entry and now you can see
it's on all the other clusters
machines on the cluster and then the
lines turn solid indicating the now this
has been replicated enough that it's
safe to execute in the state machine so
now the server can the server's can all
execute the command in their state
machines if I just another example I'll
just issue a bunch more requests we'll
put say three requests in server and
start and again you'll see all of those
requests get replicated across all the
machines and then eventually they can be
executed by the state machines and then
finally if we crash this server so I'm
just going to stop this server for some
reason it stops responding then those
other servers have their little timers
that are going to fire and eventually it
looks like server two is going to
timeout first and when it does that then
it will try and become leader it'll send
out a request and come back and then it
becomes later and takes over the cluster
okay that's once over very lightly just
to give you the basic flavor of what
happens now I'm going to go back in and
talk about in more detail how all this
stuff works okay so the first thing you
need to know is about server states a
server can be in any one of three states
at any given time
normally at any given time there will be
one server that's leader and everybody
else will be in what's called follower
state followers are completely passive
they do nothing they take no actions on
their own the only thing they do is that
they expect to hear from a leader
occasionally if they don't get any
message at all from a leader then they
get anxious and they become a candidate
that's the set in the middle state at
which point they try and win a lecture
to become the leader and if they do an
election then they become the leader and
they stay leader until various things
that can happen that makes them step
down and go back to being followers
again so those are the three states
there are only two messages to requests
that are used in the system we use a
remote procedure call mechanism so
request response exchanges and there's
only two are PCs in the raft system
again part of I want to minimize
complexity minimize the number of
different things there's one that's used
by candidates in which they ask for
votes to try and become leader
and then one used by the leader to take
entries from their log and replicate
those entries out to other machines in
the cluster and actually the the append
entries RPC is also used as a heartbeat
mechanism you can send it out without
any actual log entries in it except
everything but the entries and that's
used to serve as a heartbeat in the
cluster so this is a server States on
the the two art pcs that you'll see in
the protocol that follows now there's
one other piece of information I need to
tell you about before I start going into
the the specific algorithm and that's
terms in order to do consensus you have
to have a mechanism for detecting
obsolete information things things can
become obsolete that you need to ignore
the best example is a some one was
leader and for some reason they didn't
respond and so we elected a new leader
and they're no longer leader so we need
to detect that that's an obsolete leader
don't take instructions from that leader
anymore the mechanism for that in Raths
is something called a term so you could
think of time as being divided up into
these periods of time
call terms they have numbers that
increase over time and each term starts
with an election for a leader and then
if the election was successful then that
leader rules for that's the green period
for some period of time until that
leader crashes or something bad happens
and then we start the next term so most
terms will have an election and then a
reign of the leader occasionally there
may be a term where that we tried to
elect the leader in the election fail
because of a split vote in which case
that term will end with no leader and
we'll go to the next term and try again
to get a leader so I've drawn the terms
in this very nice
clean fashion but in fact there is no
global view of terms every server is
actually going to be observing these
term changes at slightly different times
and each server keeps for itself its
idea of what it thinks the current term
is so the there may be different servers
at at the same time think that the
current term is different values they
haven't they're out of date in order to
keep this information up to date the
servers are constantly exchanging this
information whenever one server makes
to another it includes its term in the
request and the response includes the
term of the other machine and so if
either machine sees that the other
machine has a later term if you ever see
that my term is not the latest of the
whole system then that machine instantly
has an identity crisis base it updates
its term to the latest one and it
immediately becomes a follower because
I'm stupid I know nothing there are
others that know more than me I will
become a drone follower now please give
me instructions Oh Lord so they
immediately stepped out and then by the
way if server receives an RPC with an
old term it doesn't do what the RPC says
never never execute an RPC it simply
responds with error saying dude I think
you got the wrong to get the wrong term
time to become a follower again so again
the key thing about terms is that they
allow us to find out when things are
obsolete to find out the best most
important latest information and that
over time they converge as the machines
talk to each other if nobody's failed
with no crashes eventually all the terms
will converge on the latest value
everybody will agree any questions so
far
yeah
good question to what degree is time
factors so there is no real notion of
well the only notion of time is first of
all these terms so there's no shared
clock among all these machines they do
have a notion of time advancing in the
way the terms change and machines have
to be able to measure time enough to
timeouts so I have to do I feel to tell
if a particular roughly a particular
amount of elapsed time has occurred but
there is no shared central clock
definitely that wouldn't that would
cause problems if we depended on that so
I would say so you don't typically you
don't assume that machines clocks are
synchronized but typically you can
assume that they all have clocks that
are advancing within some rough
equivalent rate they won't advance at
exactly the same rate but that you know
what appears to be a half a second on
one machine is probably not going to
appear to be ten seconds on another
machine so Paxos does not assume even
that but then Paxos is not live that is
pact so does not guarantee to make
progress and so actually one of the
fundamental rules if you've studied for
example the FLP theorem what the F felt
to me FOP theorem says different things
of different people what it says to me
is you have to use time in distributed
systems if you want to coordinate them
you can't if you can't use time you
can't coordinate distributed systems
gift of some notion of the passage of
time and elapsed time yep
right all outside of directions have to
go to the leader if you talk to a
follower by mistake the followers say
dude I don't know nothing I'm just a
follower talk to the leader and
everything it'll it will redirect it to
the leader yeah it's not going to become
a performance bottleneck yes that's sort
of a fundamental thing about consensus
though my definition we're going to have
to do things on a majority of the
clusters machines anyway and so you are
you are sort of limited by the
capability of a single machine if you're
the only way to make consensus
algorithms scalable is to partition the
system into separate consensus clusters
within a cluster unfortunately there is
really it's it is fundamentally not
scalable yep
can the operation be pipeline yeah they
can be heavily pipeline where server can
be receiving a batch from one set of
clients and replicating others yeah
you'll see more about that as I go
through the details okay so now I think
we can talk about leader election it's
really very simple
I mentioned that each each follower has
a timeout when the timeout expires then
it becomes a candidate and when it
becomes a candidate it immediately
increments its current term casts a vote
for itself and then it sends out these
requests vote remote procedure calls to
all of the other servers on the clusters
but broadcasts are requests like please
give me your vote when that after that
then one of three things will happen
first the most common thing is it will
get back votes from other servers and
once it gets back enough votes to
constitute a majority of the cluster
then it becomes the leader takes over
start sending heartbeats itself two
heartbeats keep the cluster under
control so nobody else becomes leader
and we're off and running that's the
first possibility the second possibility
is that somebody else actually timed out
and went around and gathered votes and
in fact before we can get any votes they
come and we get a you know heartbeat
from them saying I'm leader then when
that happens you instantly believe them
sir you step down go back to being a
follower and listen to orders from that
leader and then the third
ability is you don't get enough votes
I'll talk about that more on the next
slide but if a whole bunch of servers
try and become candidates at the same
time they might split the vote and so
nobody can get a majority then what
happens if a certain amount of time goes
by and nobody has gotten the majority
then the various machines will timeout
which basically go back start a new
election bump the term to the next term
and repeat the whole process until
eventually somebody gets elected yes
when will the other nodes vote fully
they'll vote as soon as they get a
request so they don't have to wait for a
time out if somebody asks you for your
vote you'll see on the next slide and
you haven't yet given out a vote for
that term you immediately give them your
vote how do you notice switch well if
there's a current leader presumably it
had the old same values our old term and
so when we bump our term to a higher
term number so for example it was term
from the leaders so we bumped our term
will ask for a vote from the old leader
that old leader will see that we're
it immediately goes back to being a
follower and then it said Oh somebody
wants my vote sure you could have my
change them the new term the key thing
there is that new term allows us to
replace the old view of the world with a
new view of the world yes
which condition will a candidate become
follower its if we start it's possible
that two machines both become candidate
exactly the same time and one of them
collects votes really quickly becomes
leader so the other one is trying to
collect votes but doesn't get them it'll
eventually receive a request from the
new leader saying I'm leader when it
sees that it goes ah somebody else has
already won the election I'm not going
to be leader and so it will go back to
being follower again how do you
guarantee progress yes let me go on and
talk about the correctness of this so
there sure
yeah I don't have time to talk about
cluster membership today unfortunately
but at any given time all of the servers
know the entire membership of the
cluster as an additional mechanism for
how do you manage that how you add nodes
and take nodes out for that you'll have
to look at the paper but I won't have
time to talk about that so assume for
now all of the nodes know all of the
servers in the cluster know who all the
other servers are right they did not all
need to be alive only majority of them
need to be alive as you'll see
so election correctness means two things
it has to be safe and has to be live
safety means nothing bad happens and
liveness means something good happens so
the safety property for this is that we
don't want more than one machine to be
able to get elected leader at any given
term so this is pretty easy to do
because each server will only give one
vote for a given term and it saves that
information on disk SB persistence so if
that machine crashes and restarts it
won't accidentally give out same vote to
different votes for the same term and
then if we have to have a majority to
win election so given that if we have
five servers and one of the candidates
has gotten three of those clearly no
other candidate can get three and so
we're guaranteed safety with the these
simple properties however we're not
guaranteed liveness with these
properties that is as possible though
that say three servers could become
candidates and they split all the votes
and nobody has a majority and then it's
possible they could all timeout they all
start up again repeat this and what
keeps this process from repeating
forever and never actually electing a
leader and the answer is random numbers
are your friend actually one of the
lessons from distributed systems
distributed systems need to use
randomization if you want to build
large-scale systems so the way we do it
is we pick the election time out from
random from a random number chosen in an
on the visualization that all of those
timeout rings were different sizes on
the different machines it's because they
all pick their timeouts differently and
so the idea of this is that with
randomization one server will timeout
first and before anybody else times out
that first server will have
made its request for votes in one
election and everything will be settled
so this works really well as long as the
time range for your time out is a lot
larger than time it takes to do a
broadcast between machines so that's
actually works very reliably in practice
you rarely it really takes more than one
cycle to get a leader elected and you
can do this left with pretty short time
so two timeouts down on the range of a
few milliseconds works fine now this was
an area where it turned out that we
spent a lot of time trying to find an
understandable way of doing this and the
randomized approach ended up being way
way simpler to understand than the
alternative we thought about was
initially we used ranking approaches
kind of like Paxos where Paxos has these
proposal numbers and the biggest
proposal number wins we tried to somehow
rank our servers so the one with the
high if there's competition the one with
the highest number wins and then people
would vote for whoever was highest but
this got really complicated it was very
hard to make it live to make it actually
work if machines crashed and we kept
adding special case after special case
and there were more problems and finally
we hit on this idea of doing
randomization and suddenly everything
got really simple just using the random
number approach all of a sudden just
pick a time out and in fact a nice thing
about this is the same approach works
the time out approach for doing the
initial time out and that also works if
we get a split vote we have to try an
election again the same mechanism solves
both of those problems
the question is what happens if somehow
your machines might get heavily loaded
so server can't respond within the
timeout value so what will happen now
which will get a server election you
probably if you want these servers to
provide good service then you probably
don't want them running on machines
where they're not gonna be able to run
for long periods of time anyhow right
because if a server stops fundamentally
the cluster may not be able to provide
service so my answer to that question I
would say my answer is don't do that
sort of like when you call the doctors
say I keep hitting myself on the head
with a hammer and might get a headache
what should I do
if your machines can't reliably respond
in some period of time then you're going
to have to increase your timeouts beyond
that there's really no choice
randomization is probably the biggest
difference yes right and we didn't
actually agree on a value here this is
really only the first step of paxos
actually if you want to compare this
leader election is kind of like the
first phase of Paxos and the replication
is kind of like the second phase of
taxes
yes the follower just votes forever asks
it first yep first-come first-served
yeah let me I think I'd better move on
and keep rolling and we're going to run
out of time and I don't want to have
time for questions on the later slides
also so now let's move on to normal
operators that's the first phase leader
election the second thing is normal
operation this you mostly saw in the
visualization so I can probably go over
this pretty quickly but once we have a
leader if a client wants the state
machine to execute command it sends that
to the leader the leader adds that
command to its own log and then it sends
out remote procedure calls to all of the
other followers asking them to append
that command to their log assuming this
all works and the leader gets enough
responses back from clients at some
point it will decide that that entry is
committed and I'll talk more about that
in the next slide but committed if it
entry is committed that means it's
replicated enough in enough logs that it
should be safe for the state machines to
execute the command when that happens
then the leader immediately executes the
command in its own state machine and
then it can respond to the client and
then over time the leader tells all the
followers about that - each time it
sends an append entries RPC it tells the
followers all entries up through this
one in the log or committed and then
once the followers find that out then
they can execute the commands in their
state machines as well so what happens
if a filer crashes or run slowly the
answer is the leader just keeps retrying
over and over and over again until
eventually presumably that client comes
back up and works well enough that the
that this can eventually succeed so the
whole cluster as you're going to see can
continue to make progress as long as we
have a majority of the nodes up any
number less than a majority can be down
and by the way this performance is
optimal in the common case the least you
can possibly do is to have one
successful remote procedure call to each
of a majority of the servers so we know
we've got the data replicated across a
majority of the cluster
now let me just oh yeah question
how can we send the result back here all
we need is for the command to have been
stored on the followers we don't need
for the followers to have executed it in
their state machines yet they can do
that any time later on and if you'll see
if the leader crashes the followers will
eventually find out from from the new
leader that this command has been
committed so all we need is for the
command to be stored in the logs of a
majority of the clients and that makes
the command durable and safe to execute
now let me talk a little bit about what
the logs look like so I've shown here
just a sample cluster with five servers
each one with its own log and each log
entry store is a command I've just used
gets the value of Q just to show you
these are commands of some sort for the
state machines that are in the logs
there are a couple of interesting things
about the log the first one is that
every log entry stores a term so for
example this entry store is the term one
the central store is a term to the term
is that that is the number of the term
when the leader received that entry from
the client and I've color-coded the logs
just for simplicity so you can see so
all of the Green entries are entries
from term one all the yellow entries
from term two the blue from term three
so these are the this is the term number
of the leader had at its point when it
received the entry and it put the entry
in its log second thing to know about
this is the notion of commitment so what
makes an entry committed and remember
the the definition of committed is once
it's committed it's safe for a state
machine to execute that entry which
means that we know we can be guaranteed
that no other state machine will ever
execute a different entry at the same
log point we're guarantee that everybody
will do the same thing for that entry
and the answer is it's safe to do that
if the entry has been stored on a
majority of the servers by the leader of
its term so for example in this case all
of the entries up through slot eight are
because these entries this LED tree is
stored on three servers and you can see
it happened during term three we have no
no leader for term four so we know the
leader for terms restored all of us at
that point it's safe to execute I'm not
going to tell you now why I'll come back
you I think you hopefully you'll see by
the end of the talk why that's safe to
execute but as long as it's on a
majority of the servers and as long as
the leader knows now the leader for term
three can execute that so we can't
execute entries nine and ten yet because
they're not safely stored and notice we
have a server that's really a laggard
here server the fourth server here is
way behind that doesn't stop the cluster
from moving forward right so now let me
talk about safety so everything is great
as long as machines don't crash but if
machines crash fundamentally the logs
can become inconsistent they won't all
necessarily store exactly the same
information at the same points and this
is an example of a cluster for example
suppose we just elected server one to be
leader for term for the cluster might
look like this and there's many ways
this could have happened but but one way
this could have happened is server for
just crashed way back during term one
and it's been out of action ever since
so it's missing a whole bunch of entries
elected as leader for term - and it's
successfully replicated some entries as
much as it could and then it received a
bunch more on its log but it crashed
before it replicated those so those have
not been replicated they're not
committed in fact that we don't
guarantee to keep those entries in fact
those entries now those are all stale
they're obsolete they never got
successfully replicated then it looks
it replicated some entries and had a few
more that it was in the process of
replicating when it crashed so you can
see you could end up with servers that
are missing entries or have extraneous
entries this can result in tremendous
amounts of potential inconsistency in
the logs if you're not careful
and so in Raph we try to really simplify
this how can we make this really really
simple and the answer is we decided the
lead
log is always going to be correct we're
going to declare that the leaders log is
perfect it is perfection
never never anything wrong with the
leaders log and in the normal mode of
operation all inconsistencies will get
repaired no special steps needed well
I'll give you one if statement that will
basically repair all of the
inconsistencies so we're trying again to
simplify as much as we possibly can the
number the degree of non-determinism and
special cases so how do we do this the
answers it took two things to do it the
first is how do we maintain consistency
so we want in our goal in draft is to
have the highest level of consistency in
the logs keep them as close as possible
to absolutely identical we can't do it
perfect but for example we do not allow
any holes in the log can never be a
missing entry followed by entries that
are present and the logs are always
sorted in order of term nice neat never
a higher numbered term before a lower
number term in the log not allowed in
fact we guarantee what we call the log
matching property in raft this says that
if there are two entries at the same
position a log like for example entry
six two entries that have the same term
number in them then you can be
guaranteed that they also have the same
command and that the logs are completely
identical up to this point guaranteed so
this this reduces this by maintaining
this property we eliminate a whole bunch
of problems we might have to address in
trying to reconcile inconsistent logs so
have different term numbers in them and
so there's no guarantee that they hold
the same command or that the logs are
the same up to that point and by the way
this also says that we can't copy entry
violate the rule because if these
entries were the same the logs would
have to be the same up until then and
they're on so we can't move that entry
we have if we want to copy this entry
down here we have to clean up everything
beforehand
so one of the side effects of this by
the way is that if one entry is
committed all the preceding entries are
committed as well because we know that
if it entries on a majority if these
machines is on a majority of all the
other machines as well all the other
early interest as well so how do we
maintain that property it's done with a
really really simple modification to the
append entries RPC whenever the leader
sends out a log entry to another machine
so suppose the leader wants to send the
follower over here in addition to that
entry it includes the index the log
index and the term number from the
preceding entry and it's so it sends all
of the information in the red box down
to the follower and the follower
compares the term number of the
preceding entry with its own entry it
has to have an entry there and the term
has to match and if that term doesn't
match the follower rejects this RPC says
sorry I can't accept this right now now
in this particular case it does match
and so the follower will add this entry
to its log and then after the operation
the log would look like that however
imagine this case where the follower
seems to have some extra junk left over
from term one so we send this entry down
refuses to accept this RPC it leaves its
log as it is and now what the leader
does is the leader sees Isle we have an
inconsistency
it simply backs up one and tries again
with a bigger chunk of data so this next
time around it will go back and send two
entries it'll send these entries through
numbers match and so the follower will
accept these new entries put them in its
log and by the way when it over race
entries like visit clears everything
after that in the log so now the
followers log looks like that which
matches the leaders log so this simple
rule it's basically an induction step
it says once the logs get in sync this
rule guarantees you can't add on to the
log without maintaining that at
synchronization and so just the normal
mode of operation as the leader works
it's going to force every followers log
to perfectly match its own log right the
problem with Paxos is really hard to
stitch together all these different
commitments it's much easier to think
about a log that we do nice clean up
ends where we could have relatively
deterministic consistent log yes right
it's it's it so here's the idea was to
make the algorithm simple enough you
could just see that it worked you know
that's sort of that you your intuition
that it works is natural and correct and
you can kind of verify correctness that
way each node needs a unique number the
only unique numbers you need of these
term numbers which it pardon there
cannot be two so there cannot be two
leaders for the same term because that's
our leader election prevents that from
happening so the nodes correspond to the
logs right so here we have one no that's
the leader in one no this is the
follower each log entry has to have a
unique index here yes that's true the
log has indexes that increment naturally
yeah
very good question how do we know this
log hasn't been executed by the state
machine hold back for one more slide so
because we've been assuming the leaders
log is correct so as long as the leaders
log is correct right then then if you
follower differs from it the follower
must be wrong okay so now your question
is well how do we know the leaders log
is correct and so that's the last piece
of safety which we call the leader
completeness property so what we want to
guarantee is that once the log entry has
been committed whenever a machine is
elected leader it will hold that log
entry so as long as the leader always
holds all the committed entries and then
it copies it's logged all of the
followers we know that we're never going
to lose a committed entry we'll never
overwrite a committed entry so then the
question is how do we guarantee that
leaders hold all entries and the answer
is we add a little bit extra to the
election rules to do that let me show
you an example so in this slide we have
an example of five nodes I've left out
of the command so I'm just showing the
term numbers in here because that's all
it really matters in the log entries and
we're about to elect us a leader for
term four so you can see entries up
through eight have been committed you
can also see we don't want to elect
server two doesn't hold that entry we
definitely don't want to elect server
five because it holds all of these bogus
entries that would conflict with what we
think is the truth so how do we keep
that from happening and the answer is
that when a candidate requests votes it
sends out information about its last log
entry so for example the server one asks
for votes in every request it'll say by
it has term three and then the the
voting machines will compare that with
the length of their own log and they
will deny the vote if their log is more
complete than than the candidates log
and how do we decide completeness that's
based on the combination of the term and
the index so for example if my log ends
in a higher term than your log my logs
more complete if our logs and in the
same term and my log is longer than
yours then mine is more complete
so for example if server five tries to
get a vote it can't get a single vote
from the cluster because everybody is
going to compare and see oh my login is
in term three and yours is term two my
logs more complete server two can get a
vote from server five because it has a
higher term but it can't get a vote from
anybody else because this log is shorter
everybody else's so the results is this
really simple rule keeps anybody from
getting elected leader and unless they
have all the committed entries and so
that's just now validates we basically
made an assumption that leaders have to
have a lose log has to be perfect and
then we made sure that leaders log is in
fact complete if you have a network
partition as the thing about a network
partition is you still have to have a
majority of the servers to run so there
can only be one leader left in the
cluster after a partition you can't have
two separate leaders if your partition
absolutely one partition can not compute
yep yeah and I realized I'm running
really late on time so I think I'd
better let me skip through my last
couple of slide this is the main part of
the wrath I just want to talk about a
couple evaluation stuff and then I'll
wrap up and I'm happy to come back and
revisit any of the stuff in questions so
in terms of so that's the complete raft
algorithm in terms of how to evaluate it
it's been evaluate a ton of different
ways as formal proofs Diego did a part
for his dissertation a team at
code to mechanically check the whole
thing Diego has a C++ implementation
there's others as well he's looked at
the leader election pretty thoroughly
what I want to very briefly about as we
did a user study to try and validate the
understandability
so the question is how do we know it
really is easier to understand than
Paxos and the answer is we got students
in a couple of operating system classes
to participate we divide them up into
two groups
each group would learn one system by
studying a video take a test then see a
video on the other system and take a
test and then we compared the results of
that I don't have time to talk about how
we how we set this up trying to do this
fairly was really hard trying to make
the video so they really were fair and
equivalent for the system
and the quizzes were equally hard free
system you know we didn't stack the deck
and have the world's toughest paxos quiz
with a bunch of softball raft questions
to compare against that was so actually
initially we want to use different
instructors have somebody really loved
pack supposed to get the packs those
lectures and somebody who really loves
RAF to give the raft lecture the problem
was the person who was supposed to the
Paxos lecture kind of flaked out on us
and so then actually I gave it my best
shot the videos were online you can
actually look and see I think I did
actually a reasonable job at at giving a
paxos lecture and by the way I should
say a lot of the students already had
prior Paxos experience so paxos was a
little bit favorite of the experiments
so the results are this shows this their
scores on the two tests of the Paxos
score on the x-axis and the raft score
on the y-axis I'm not going to go over
all the statistical details you can look
at the paper for that but you can just
see visually there's a lot more data
points above the diagonal than below
that means students generally did better
better so I did better on draft then
linear regression where you factor out
prior Paxos experience the expectation
is students we do about twice as well on
the raft test as the paxos test and my
favorite little bit of statistics all of
these blue X's down here near the origin
what happened is that if a student
learned
paxos first they did worse on both tests
than if they learn Paxos second and that
is statistically significant we don't
know why that is but it's almost as if
learning Paxos trying to learn Paxos
gets you so confused you can't
understand any consensus algorithm and
then finally we surveyed them afterwards
it's just opinions which you thought
would be easier to implement or explain
and you can see that again RAF was
pretty strongly favored so in terms of
impact I will say this was really hard
to get published I think the idea of
understandability as a metric was hard
for program committees to get
comfortable with and
honestly I think complexity impresses
program committees making something
simpler people somehow think there's no
art or difficulty there when in fact
it's it actually is pretty hard to make
things simple but ironically on the
adoption scheme draft has been amazingly
implementations before the paper was
finally published because we released
early versions of our paper and people
just started implementing it there's
almost a hundred implementations now on
the raft homepage is out of date a lot
of them in production and furthermore
it's starting to get taught in graduate
operating system classes and I find it
ironic what does it mean when they won't
accept a paper at a conference but they
want to teach their students that paper
go figure
by now personally I'm much more happy
with the adoption stuff I'm willing to
tolerate the you know the rejection on
the left side in order to get the
adoption on the right that's personally
what I should live for is a computer
science is to have something that's
really impacting people so there's a
bunch of other information available on
this there's several parts of raft
having talked about you can go to the
paper or to Diego's dissertation
actually one humorous note is Diego's
dissertation must be motive one of the
most widely read dissertations at all of
computer science there's a raft mailing
list and people write in to the mailing
list with questions about and people say
Diego's dissertation for that and so
it's as widely cited and quoted as
people discuss the raft algorithm there
are other algorithms besides raft and
Paxos you stamp replication zookeeper
the two examples I'll let you hunt those
for more details so just to conclude I
really think that understandability is
undervalued in algorithm system design
today and I would love to see the world
spending more time on that I think it
would make our systems much better if we
design them explicitly thinking about
understandability and two things to
think about decomposition and minimizing
state space trying to this second making
a system simpler it could have quite
high impact people there's a hunger for
that you know I think raft has made it
easier for people to build consensus
systems and so a lot more people are
building consensus systems now that
wouldn't have even tried with Paxos and
the third Paxos is not going away it's
it's still it is the tiniest smallest
most concise description of consensus so
in that sense it's interesting but
I think from the standpoint of both
implementation and teaching I think raft
is better it's it's easier to understand
and a lot more complete than Texas's so
finally in case you're wondering why did
we pick the name raft plus three reasons
first is that raft stands for replicated
and fault tolerant second a raft is
something you can build out of a
collection of logs and third a raft is
something you can use to get away from
the island of paxos thank you
